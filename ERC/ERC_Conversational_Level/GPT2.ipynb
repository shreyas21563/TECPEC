{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Model, GPT2PreTrainedModel, GPT2Config\n",
    "import pickle\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/LCS2-IIITD/Emotion-Flip-Reasoning/blob/main/Dataloaders/nlp_utils.py\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "numbers = {\n",
    "    \"0\":\"zero\",\n",
    "    \"1\":\"one\",\n",
    "    \"2\":\"two\",\n",
    "    \"3\":\"three\",\n",
    "    \"4\":\"four\",\n",
    "    \"5\":\"five\",\n",
    "    \"6\":\"six\",\n",
    "    \"7\":\"seven\",\n",
    "    \"8\":\"eight\",\n",
    "    \"9\":\"nine\"\n",
    "}\n",
    "\n",
    "def remove_puntuations(txt):\n",
    "    punct = set(string.punctuation)\n",
    "    txt = \" \".join(txt.split(\".\"))\n",
    "    txt = \" \".join(txt.split(\"!\"))\n",
    "    txt = \" \".join(txt.split(\"?\"))\n",
    "    txt = \" \".join(txt.split(\":\"))\n",
    "    txt = \" \".join(txt.split(\";\"))\n",
    "    \n",
    "    txt = \"\".join(ch for ch in txt if ch not in punct)\n",
    "    return txt\n",
    "\n",
    "def number_to_words(txt):\n",
    "    for k in numbers.keys():\n",
    "        txt = txt.replace(k,numbers[k]+\" \")\n",
    "    return txt\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'_',' ',text)\n",
    "    text = number_to_words(text)\n",
    "    text = remove_puntuations(text)\n",
    "    text = ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('../../Dataset/ERC_conversational_level/train_conversation_level.json'))\n",
    "val_data = json.load(open('../../Dataset/ERC_conversational_level/val_conversation_level.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion2int = {\n",
    "    'anger': 0,\n",
    "    'joy': 1,\n",
    "    'fear': 2,\n",
    "    'disgust': 3,\n",
    "    'neutral': 4,\n",
    "    'surprise': 5,\n",
    "    'sadness': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance2vec = pickle.load(open('../../Dataset/Embeddings/sentence_transformer_utterance2vec_768.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONV_LEN = 35\n",
    "# Defined index 7 for padding\n",
    "class ERC_Dataset_Conv_Level(Dataset):\n",
    "    def __init__(self, data, utterance2vec):\n",
    "        self.data = data\n",
    "        self.utterance2vec = utterance2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        texts = [utterance['text'] for utterance in conversation]\n",
    "        emotions = [emotion2int[utterance['emotion']] for utterance in conversation]\n",
    "        text_embeddings = [torch.from_numpy(self.utterance2vec[preprocess_text(text)]) for text in texts]\n",
    "                \n",
    "        if(len(text_embeddings)<MAX_CONV_LEN):\n",
    "            num_pads = MAX_CONV_LEN - len(text_embeddings)\n",
    "            attention_mask = [1]*len(text_embeddings) + [0]*num_pads\n",
    "            text_embeddings = text_embeddings + [torch.zeros(768)]*num_pads\n",
    "            emotions = emotions + [7]*num_pads # 7 is the index for padding\n",
    "        else:\n",
    "            text_embeddings = text_embeddings[len(text_embeddings)-MAX_CONV_LEN:]\n",
    "            attention_mask = [1]*MAX_CONV_LEN\n",
    "            emotions = emotions[len(emotions)-MAX_CONV_LEN:]\n",
    "\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        emotions = torch.tensor(emotions)\n",
    "        return {\n",
    "            'text_embeddings': text_embeddings,\n",
    "            'attention_mask': attention_mask,\n",
    "            'emotions': emotions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ERC_Dataset_Conv_Level(train_data, utterance2vec)\n",
    "val_dataset = ERC_Dataset_Conv_Level(val_data, utterance2vec)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERC_GPT2(GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.GPT2 = GPT2Model(config)\n",
    "        self.classifier = nn.Linear(config.n_embd, self.num_labels)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask, labels=None):\n",
    "        outputs = self.GPT2(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        outputs = outputs.last_hidden_state.reshape(-1, 768)\n",
    "        attention_mask, labels = attention_mask.reshape(-1), labels.reshape(-1)\n",
    "        outputs = [outputs[i] for i in range(len(attention_mask)) if attention_mask[i] == 1]\n",
    "        labels = [labels[i] for i in range(len(attention_mask)) if attention_mask[i] == 1]\n",
    "        labels = torch.tensor(labels).cpu()\n",
    "        outputs = torch.stack(outputs)\n",
    "        logits = self.classifier(outputs).cpu()\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'labels': labels    \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ERC_GPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['GPT2.h.0.attn.c_attn.bias', 'GPT2.h.0.attn.c_attn.weight', 'GPT2.h.0.attn.c_proj.bias', 'GPT2.h.0.attn.c_proj.weight', 'GPT2.h.0.ln_1.bias', 'GPT2.h.0.ln_1.weight', 'GPT2.h.0.ln_2.bias', 'GPT2.h.0.ln_2.weight', 'GPT2.h.0.mlp.c_fc.bias', 'GPT2.h.0.mlp.c_fc.weight', 'GPT2.h.0.mlp.c_proj.bias', 'GPT2.h.0.mlp.c_proj.weight', 'GPT2.h.1.attn.c_attn.bias', 'GPT2.h.1.attn.c_attn.weight', 'GPT2.h.1.attn.c_proj.bias', 'GPT2.h.1.attn.c_proj.weight', 'GPT2.h.1.ln_1.bias', 'GPT2.h.1.ln_1.weight', 'GPT2.h.1.ln_2.bias', 'GPT2.h.1.ln_2.weight', 'GPT2.h.1.mlp.c_fc.bias', 'GPT2.h.1.mlp.c_fc.weight', 'GPT2.h.1.mlp.c_proj.bias', 'GPT2.h.1.mlp.c_proj.weight', 'GPT2.h.10.attn.c_attn.bias', 'GPT2.h.10.attn.c_attn.weight', 'GPT2.h.10.attn.c_proj.bias', 'GPT2.h.10.attn.c_proj.weight', 'GPT2.h.10.ln_1.bias', 'GPT2.h.10.ln_1.weight', 'GPT2.h.10.ln_2.bias', 'GPT2.h.10.ln_2.weight', 'GPT2.h.10.mlp.c_fc.bias', 'GPT2.h.10.mlp.c_fc.weight', 'GPT2.h.10.mlp.c_proj.bias', 'GPT2.h.10.mlp.c_proj.weight', 'GPT2.h.11.attn.c_attn.bias', 'GPT2.h.11.attn.c_attn.weight', 'GPT2.h.11.attn.c_proj.bias', 'GPT2.h.11.attn.c_proj.weight', 'GPT2.h.11.ln_1.bias', 'GPT2.h.11.ln_1.weight', 'GPT2.h.11.ln_2.bias', 'GPT2.h.11.ln_2.weight', 'GPT2.h.11.mlp.c_fc.bias', 'GPT2.h.11.mlp.c_fc.weight', 'GPT2.h.11.mlp.c_proj.bias', 'GPT2.h.11.mlp.c_proj.weight', 'GPT2.h.2.attn.c_attn.bias', 'GPT2.h.2.attn.c_attn.weight', 'GPT2.h.2.attn.c_proj.bias', 'GPT2.h.2.attn.c_proj.weight', 'GPT2.h.2.ln_1.bias', 'GPT2.h.2.ln_1.weight', 'GPT2.h.2.ln_2.bias', 'GPT2.h.2.ln_2.weight', 'GPT2.h.2.mlp.c_fc.bias', 'GPT2.h.2.mlp.c_fc.weight', 'GPT2.h.2.mlp.c_proj.bias', 'GPT2.h.2.mlp.c_proj.weight', 'GPT2.h.3.attn.c_attn.bias', 'GPT2.h.3.attn.c_attn.weight', 'GPT2.h.3.attn.c_proj.bias', 'GPT2.h.3.attn.c_proj.weight', 'GPT2.h.3.ln_1.bias', 'GPT2.h.3.ln_1.weight', 'GPT2.h.3.ln_2.bias', 'GPT2.h.3.ln_2.weight', 'GPT2.h.3.mlp.c_fc.bias', 'GPT2.h.3.mlp.c_fc.weight', 'GPT2.h.3.mlp.c_proj.bias', 'GPT2.h.3.mlp.c_proj.weight', 'GPT2.h.4.attn.c_attn.bias', 'GPT2.h.4.attn.c_attn.weight', 'GPT2.h.4.attn.c_proj.bias', 'GPT2.h.4.attn.c_proj.weight', 'GPT2.h.4.ln_1.bias', 'GPT2.h.4.ln_1.weight', 'GPT2.h.4.ln_2.bias', 'GPT2.h.4.ln_2.weight', 'GPT2.h.4.mlp.c_fc.bias', 'GPT2.h.4.mlp.c_fc.weight', 'GPT2.h.4.mlp.c_proj.bias', 'GPT2.h.4.mlp.c_proj.weight', 'GPT2.h.5.attn.c_attn.bias', 'GPT2.h.5.attn.c_attn.weight', 'GPT2.h.5.attn.c_proj.bias', 'GPT2.h.5.attn.c_proj.weight', 'GPT2.h.5.ln_1.bias', 'GPT2.h.5.ln_1.weight', 'GPT2.h.5.ln_2.bias', 'GPT2.h.5.ln_2.weight', 'GPT2.h.5.mlp.c_fc.bias', 'GPT2.h.5.mlp.c_fc.weight', 'GPT2.h.5.mlp.c_proj.bias', 'GPT2.h.5.mlp.c_proj.weight', 'GPT2.h.6.attn.c_attn.bias', 'GPT2.h.6.attn.c_attn.weight', 'GPT2.h.6.attn.c_proj.bias', 'GPT2.h.6.attn.c_proj.weight', 'GPT2.h.6.ln_1.bias', 'GPT2.h.6.ln_1.weight', 'GPT2.h.6.ln_2.bias', 'GPT2.h.6.ln_2.weight', 'GPT2.h.6.mlp.c_fc.bias', 'GPT2.h.6.mlp.c_fc.weight', 'GPT2.h.6.mlp.c_proj.bias', 'GPT2.h.6.mlp.c_proj.weight', 'GPT2.h.7.attn.c_attn.bias', 'GPT2.h.7.attn.c_attn.weight', 'GPT2.h.7.attn.c_proj.bias', 'GPT2.h.7.attn.c_proj.weight', 'GPT2.h.7.ln_1.bias', 'GPT2.h.7.ln_1.weight', 'GPT2.h.7.ln_2.bias', 'GPT2.h.7.ln_2.weight', 'GPT2.h.7.mlp.c_fc.bias', 'GPT2.h.7.mlp.c_fc.weight', 'GPT2.h.7.mlp.c_proj.bias', 'GPT2.h.7.mlp.c_proj.weight', 'GPT2.h.8.attn.c_attn.bias', 'GPT2.h.8.attn.c_attn.weight', 'GPT2.h.8.attn.c_proj.bias', 'GPT2.h.8.attn.c_proj.weight', 'GPT2.h.8.ln_1.bias', 'GPT2.h.8.ln_1.weight', 'GPT2.h.8.ln_2.bias', 'GPT2.h.8.ln_2.weight', 'GPT2.h.8.mlp.c_fc.bias', 'GPT2.h.8.mlp.c_fc.weight', 'GPT2.h.8.mlp.c_proj.bias', 'GPT2.h.8.mlp.c_proj.weight', 'GPT2.h.9.attn.c_attn.bias', 'GPT2.h.9.attn.c_attn.weight', 'GPT2.h.9.attn.c_proj.bias', 'GPT2.h.9.attn.c_proj.weight', 'GPT2.h.9.ln_1.bias', 'GPT2.h.9.ln_1.weight', 'GPT2.h.9.ln_2.bias', 'GPT2.h.9.ln_2.weight', 'GPT2.h.9.mlp.c_fc.bias', 'GPT2.h.9.mlp.c_fc.weight', 'GPT2.h.9.mlp.c_proj.bias', 'GPT2.h.9.mlp.c_proj.weight', 'GPT2.ln_f.bias', 'GPT2.ln_f.weight', 'GPT2.wpe.weight', 'GPT2.wte.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2', num_labels=7)\n",
    "model = ERC_GPT2.from_pretrained('gpt2', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"wandb_login_key\")\n",
    "# wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='TECPEC', name='GPT2_Conv_Level', config={\n",
    "#     'Embedding': 'Sentence-Transformer',\n",
    "#     'Level': 'Conversation Level',\n",
    "#     'Epochs': epochs,\n",
    "#     'Optimizer': 'AdamW',\n",
    "#     'Learning Rate': 1e-5,\n",
    "#     'Batch Size': 16\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 3/78 [00:12<05:06,  4.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs_embeds\u001b[38;5;241m=\u001b[39mtext_embeddings, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39memotions)\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     11\u001b[0m train_pred\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39margmax(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\shrey\\.conda\\envs\\TECPEC\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shrey\\.conda\\envs\\TECPEC\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_pred, train_true, train_loss = [], [], 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        text_embeddings, attention_mask, emotions = batch['text_embeddings'].to(device), batch['attention_mask'].to(device), batch['emotions'].to(device)\n",
    "        outputs = model(inputs_embeds=text_embeddings, attention_mask=attention_mask, labels=emotions)\n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_pred.extend(torch.argmax(outputs['logits'], 1).tolist())\n",
    "        train_true.extend(outputs['labels'].tolist())\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader) \n",
    "    model.eval()\n",
    "    val_pred, val_true, val_loss = [], [], 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            text_embeddings, attention_mask, emotions = batch['text_embeddings'].to(device), batch['attention_mask'].to(device), batch['emotions'].to(device)\n",
    "            outputs = model(inputs_embeds=text_embeddings, attention_mask=attention_mask, labels=emotions)\n",
    "            loss = outputs['loss']\n",
    "            val_pred.extend(torch.argmax(outputs['logits'], 1).tolist())\n",
    "            val_true.extend(outputs['labels'].tolist())\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    train_report = classification_report(train_true, train_pred, target_names=emotion2int.keys(), zero_division=0)\n",
    "    val_report = classification_report(val_true, val_pred, target_names=emotion2int.keys(), zero_division=0)\n",
    "\n",
    "    train_report_dict = classification_report(train_true, train_pred, target_names=emotion2int.keys(), output_dict=True, zero_division=0)\n",
    "    val_report_dict = classification_report(val_true, val_pred, target_names=emotion2int.keys(), output_dict=True, zero_division=0)\n",
    "    # wandb.log({\n",
    "    #     'train_loss': train_loss,\n",
    "    #     'val_loss': val_loss,\n",
    "    #     'train_accuracy': train_report_dict['accuracy'],\n",
    "    #     'val_accuracy': val_report_dict['accuracy'],\n",
    "    #     'Macro train_f1': train_report_dict['macro avg']['f1-score'],\n",
    "    #     'Macro val_f1': val_report_dict['macro avg']['f1-score'],\n",
    "    #     'Weighted train_f1': train_report_dict['weighted avg']['f1-score'],\n",
    "    #     'Weighted val_f1': val_report_dict['weighted avg']['f1-score'],\n",
    "    # })\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "    print(f\"Train Report: \\n{train_report}\")\n",
    "    print(f\"Val Report: \\n{val_report}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TECPEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
